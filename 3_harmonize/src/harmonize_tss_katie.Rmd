---
title: "TSS"
author: "Katie Willi"
format: 
  html:
    code-fold: true
    code-summary: "Show the code"
#page-layout: full
fig-width: 15
editor: visual
execute:
  echo: true
  warning: false
  error: false
  message: false
---

This workflow assumes that the following steps will be included in the "pre" workflow:

```{r}
library(kableExtra)
library(feather)
library(tidyverse)
library(knitr)
library(lubridate)
library(forcats)
library(rvest)
library(scales)
library(ggthemes)

#COLOR-BLIND PALLETTE:#
#show_col(colorblind_pal()(8))


site_url <- 'https://help.waterdata.usgs.gov/parameter_cd?group_cd=%'

tables <- read_html(site_url) %>%
  html_nodes('table') %>%
  html_table()

pcodes <-tables[[1]] %>%
  janitor::clean_names() %>%
  mutate(parm_cd=str_pad(as.character(parameter_code), 5, pad = "0"))

raw_tss <- read_feather('raw_tss.feather') %>% 
  filter(CharacteristicName %in% c("Total Suspended Particulate Matter","Total suspended solids")) %>%
  dplyr::rename(date=ActivityStartDate,
                # want to preserve original characteristicname to be able to explore variety in their original charname
                #parameter=CharacteristicName,
                parm_cd=USGSPCode,
                units=ResultMeasure.MeasureUnitCode,
                SiteID=MonitoringLocationIdentifier,
                org=OrganizationFormalName,
                org_id=OrganizationIdentifier,
                time=ActivityStartTime.Time,
                value=ResultMeasureValue,
                sample_method=SampleCollectionMethod.MethodName,
                analytical_method=ResultAnalyticalMethod.MethodName,
                particle_size=ResultParticleSizeBasisText,
                date_time=ActivityStartDateTime,
                media=ActivityMediaName,
                # Matt has different definitions of things in light partitioning and aquasat!
                # And, he doesn't include this one in AquaSat.
                #waterbody_type = ResolvedMonitoringLocationTypeName,
                type=ActivityMediaSubdivisionName,
                sample_depth=ActivityDepthHeightMeasure.MeasureValue,
                sample_depth_unit=ActivityDepthHeightMeasure.MeasureUnitCode,
                fraction=ResultSampleFractionText,
                status=ResultStatusIdentifier,
                field_comments=ActivityCommentText,
                lab_comments=ResultLaboratoryCommentText,
                result_comments=ResultCommentText) %>%
  # link up USGS p-codes. and their common names can be useful for method lumping:
  left_join(pcodes, by = 'parm_cd') %>%
  mutate(year = year(date),
         units = trimws(units)) %>%
  # only 1970 beyond
  filter(year >= 1970) %>%
  # only lakes (for light partitioning)
  #filter(grepl("Lake", waterbody_type, ignore.case = T)) %>%
  # water only
  filter(media %in% c("Water","water")) %>%
  # only MM approved water types
  filter(type %in% c('Surface Water','Water','Estuary')|is.na(type)) %>%
  # no failure-related field comments, slightly different list of words than lab and result list 
  # (not including things that could be used to describe field conditions like "warm", "ice", etc.):
  filter(!grepl("fail|suspect|error|beyond accept|interference|questionable|outside of accept|problem|contaminat|improper|violation|invalid|unable|no test|cancelled|instrument down|no result|time exceed|not accept|QC EXCEEDED", field_comments, ignore.case = T) | is.na(field_comments)) %>% 
  # no failure-related lab comments:
  filter(!grepl("fail|suspect|error|beyond accept|interference|questionable|outside of accept|problem|contaminat|improper|warm|violation|invalid|unable|no test|cancelled|instrument down|no result|time exceed|not accept|QC EXCEEDED|not ice|ice melt|PAST HOLDING TIME", lab_comments, ignore.case = T) | is.na(lab_comments)) %>% 
  # no failure-related result comments:
  filter(!grepl("fail|suspect|error|beyond accept|interference|questionable|outside of accept|problem|contaminat|improper|warm|violation|invalid|unable|no test|cancelled|instrument down|no result|time exceed|not accept|QC EXCEEDED|not ice|ice melt|PAST HOLDING TIME", result_comments, ignore.case = T) | is.na(result_comments)) %>% 
  # no failure-related value comments:
  filter(!grepl("fail|suspect|error|beyond accept|interference|questionable|outside of accept|problem|contaminat|improper|warm|violation|invalid|unable|no test|cancelled|instrument down|no result|time exceed|not accept|QC EXCEEDED|not ice|ice melt|PAST HOLDING TIME", value, ignore.case = T) | is.na(value)) %>%
  # no failure-related detection comments:
  filter(!grepl("fail|suspect|error|beyond accept|interference|questionable|outside of accept|problem|contaminat|improper|warm|violation|invalid|unable|no test|cancelled|instrument down|no result|time exceed|not accept|QC EXCEEDED|not ice|ice melt|PAST HOLDING TIME", ResultDetectionConditionText, ignore.case = T) | is.na(ResultDetectionConditionText)) %>%
  # index for being able to track each sample
  rowid_to_column(.,"index")

gc()

# not doing this step anymore, relates to my chat about this.
# # samples that don't have ANY meaningful result data 
# no_data_samples <- filter(raw_tss, (is.na(value) & is.na(units) & is.na(lab_comments) & is.na(field_comments) & is.na(result_comments) & is.na(ResultDetectionConditionText) & is.na(ResultMeasureValue_original)))

# remove samples that have no values and no lab/result metadata
# raw_tss <- raw_tss %>%
#   filter(!index %in% no_data_samples$index)
```

# Analytical Method Lumping

```{r}
# DATA LUMPING

# what analytical methods are listed for TSS?
raw_tss %>%
  group_by(analytical_method) %>%
  summarize(count=dplyr::n()) %>%
  arrange(desc(count)) %>%
  #mutate(grouped=ifelse(is.na(grouped),"Not captured by aggreagting mechanism",grouped)) %>%
  kable(.,'html',caption='Analytical Methods') %>%
  kable_styling(position='center') %>%
  scroll_box(width='800px',height='500px')

# start "lumping"
aggregated_methods <- raw_tss %>%
  mutate(method_status = case_when(
    # "GOld standard" method mostly via analytical_method field:
    grepl("2540D|2540 D|105|103|160.2",
          analytical_method, ignore.case = T) |
      # ... also use USGS p-codes to identify the data performed with "gold standard" method:
      grepl("Suspended solids, water, unfiltered, milligrams per liter|Suspended solids, dried at 105 degrees Celsius, water|Suspended solids dried at 105 degrees Celsius, water, unfiltered", 
          parameter_name_description, ignore.case = T) ~ "SM 2540 B/EPA 160.2",
    # This one seems appropriate but is heated at 110 deg Celsius:
    grepl("Suspended solids dried at 110 degrees Celsius, water, unfiltered", 
          parameter_name_description, ignore.case = T) ~ "TSS USGS 110",
    # This one may be appropriate to still consider:
    grepl("2540 F|Settlable Solids|Settleable",
          analytical_method, ignore.case = T) ~ "Settleable Solids ",
     # not what we want
    grepl("2540C|2540 C|Total Dissolved|160.1|TDS|TOTAL FILTRATABLE RESIDUE",
          analytical_method, ignore.case = T) ~ "Nonsensical",#"Total Dissolved Solids",
    # not what we want
    grepl("160.4|2540 E|Ashing|Volatile Residue",
          analytical_method, ignore.case = T) ~ "Nonsensical",#"Volatile Residue",
    # not what we want
    grepl("Percent Solids|Total Solids|2540B|Total, Fixed and Volatile",
          analytical_method, ignore.case = T) ~ "Nonsensical",#"Total Solids",
    # clearly TSS, but not exactly sure how it was performed
    grepl("Nonfilterable Solids|Non-filterable Residue by Filtration and Drying|Total Nonfilterable Residue|RESIDUE, TOTAL NONFILTRABLE|Non-Filterable Residue - TSS|Total Suspended Solids in Water|Total Suspended Solids|TOTAL NONFILTRATABLE RESIDUE|Suspended-Sediment in Water|Residue Non- filterable (TSS)|TSS|Residue by Evaporation and Gravimetric",
          analytical_method, ignore.case = T) ~ "Ambiguous TSS",
    # Things we def want to remove:
    grepl("Oxygen|Nitrogen|Ammonia|Metals|E. coli|Coliform|Carbon|Anion|Cation|Phosphorus|Silica|PH|HARDNESS|Nutrient|Turbidity|Temperature|Nitrate|Conductance|Conductivity|Alkalinity|Chlorophyll|SM |EPA |2540 G", analytical_method, ignore.case = T) ~ "Nonsensical",
      # John Gardner recommended removals (including intentional misspelling on unknown!)
    grepl("UNKOWN|SSC by filtration (D3977;WI WSC)|Sediment conc by evaporation|Historic|Filterable Residue - TDS|Cheyenne River Sioux Tribe Quality Assurance Procedures", analytical_method, ignore.case = T) ~ "Nonsensical")) %>%
  # obviously not WRONG, but still highly ambiguous. Includes things like local SOPs , ot known, etc.
  mutate(method_status=ifelse(is.na(method_status),"Ambiguous", method_status)) %>%
  # These fractions don't make sense for TSS, so should be removed.
  # I feel that many of the remaining fractions are open to interpretation, and don't want to 
  # filter them out.
  filter(!fraction %in% c('Fixed','Volatile','Dissolved','Acid Soluble'))
  
```

### Methods Breakdown

Aggregated Methodologies:

```{r}
aggregated_methods %>%
  group_by(analytical_method, method_status) %>%
  summarize(count=dplyr::n()) %>%
  arrange(desc(method_status)) %>%
  kable(.,'html',caption='Grouped Analytical Methods') %>%
  kable_styling(position='center') %>%
  scroll_box(width='800px',height='500px')
```

Sample Fraction Breakdown:

These leftover fractionations make me nervous. I could see someone putting any of the reamining fractions, and it being defensible.
```{r}
aggregated_methods %>%
  group_by(method_status, fraction) %>%
  summarize(count=dplyr::n()) %>%
  arrange(desc(method_status)) %>%
  kable(.,'html',caption='Fractionation by Method') %>%
  kable_styling(position='center') %>%
  scroll_box(width='800px',height='500px')
```

```{r}
aggregated_methods %>%
  group_by(method_status) %>%
  summarize(count = n()) %>%
  arrange(count) %>%
  ggplot()+
  geom_col(aes(x = fct_reorder(method_status,count),y = count, fill = method_status)) +
  geom_text(aes(label = count, y = count/2, x = fct_reorder(method_status,count), 
                color = method_status),
            position = position_dodge(0.5),
            vjust = 0) +
  ylab("Count") +
  xlab("") +
  coord_flip() +
  theme_bw() +
  scale_fill_manual(values=c("#CC79A7","#E69F00","#56B4E9","#009E73","#0072B2","grey")) + 
  scale_color_manual(values=c("black","black","black","black","black","black","black")) + 
  scale_y_continuous(labels = comma)+
  theme(legend.position = "none",
        text = element_text(size = 20))

gc()
```

It is clear that the standard TSS analytical method (EPA 160.2/SM 2540) is the most common across grouped methods (at `r aggregated_methods %>% mutate(total=n()) %>% group_by(method_status) %>% filter(method_status=="SM 2540 B/EPA 160.2") %>% summarize(perc=(n()/total)*100) %>% distinct() %>% ungroup() %>% select(perc) %>% round(digits=3) %>% paste()`%).

#### TSS particle size fractionation

In Matt's original code, he included SSC. John Gardner suggests we DO NOT include SSC. Therefore, this particle size fractionation/percentage value harmonization is unnecessary.

```{r, fig.width=5}
#Select only units for %
tss.p <- aggregated_methods %>%
  filter(units == '%') 
#look at the breakdown of particle sizes
tss.p %>%
  group_by(particle_size) %>%
  summarize(count=n()) %>%
  kable(.,'html',caption='All particle size fractions and their count') %>%
  kable_styling() %>%
  scroll_box(width='600px',height='400px')
#Keep only the sand fraction data (~50% of the data)
sand.harmonized  <- tss.p %>%
  filter(particle_size %in%  c('< 0.0625 mm','sands')) %>%
  mutate(conversion=NA,
         harmonized_parameter='p.sand',
         harmonized_value=value,
         harmonized_unit='%')
```

### TSS depth of sampling

For TSS some sites also have the water depth of sample, which is very useful for validating whether or not the sample will reflect satellite observation of the same water parcel. However, most of the data doesn't have this depth of sampling data and it requires a bit of its own munging, since the sampling depth comes down in a range of units. Here we make the choice to not filter by depth of sample, but we show a histogram of sampling depths for sites that do have it. Over half are near surface (< 5 m).

```{r depth breakdown}
#Define a depth lookup table to convert all depth data to meters. 
depth.lookup <- tibble(sample_depth_unit=c('cm','feet','ft','in','m','meters','None'),
                       depth_conversion=c(1/100,.3048,.3048,0.0254,1,1,NA)) 
#Join depth lookup table to tss data
tss.depth <- inner_join(aggregated_methods,depth.lookup,by=c('sample_depth_unit')) %>%
  #Some depth measurements have negative values (assume that is just preference)
  #I also added .01 meters because many samlples have depth of zero assuming they were
  # taken directly at the surface
  mutate(harmonized_depth=abs(as.numeric(sample_depth)*depth_conversion)+.01)
# We lose lots of data by keeping only data with depth measurements
print(paste('If we only kept samples that had depth information we would lose',round((nrow(aggregated_methods)-nrow(tss.depth))/nrow(aggregated_methods)*100,1),'% of samples'))
ggplot(tss.depth,aes(x=harmonized_depth)) + 
  geom_histogram(bins=100) + 
  scale_x_log10(limits=c(0.01,10^3),breaks=c(.1,1,10,100)) 
gc()
```

### PLAYIN' WITH VALUES

Flagging and utilizing non-detects. First, subset all fields in the value column that are not numeric (i.e., values that include words, or `<`/`>`/`*` characters).

```{r}
funky_vals <- aggregated_methods %>%
  # only want NAs and character value data:
  filter(is.na(value)) %>%
  # if the value is na BUT there is non detect language in the comments...  
  mutate(mdl_vals = ifelse((is.na(ResultMeasureValue_original) & 
                             (grepl("non-detect|not detect|non detect|undetect|below", lab_comments, ignore.case=TRUE) | 
                              grepl("non-detect|not detect|non detect|undetect|below", result_comments, ignore.case=TRUE) |
                              grepl("non-detect|not detect|non detect|undetect|below", ResultDetectionConditionText, ignore.case=TRUE))) |
  #.... OR, there is non-detect language in the value column itself....
                              grepl("non-detect|not detect|non detect|undetect|below", ResultMeasureValue_original, ignore.case=TRUE),
  #... use the DetectionQuantitationLimitMeasure.MeasureValue value.
                           DetectionQuantitationLimitMeasure.MeasureValue,
  # if there is a `<` and a number in the values column...
                     ifelse(grepl("0|1|2|3|4|5|6|7|8|9", ResultMeasureValue_original) & grepl("<", ResultMeasureValue_original),
                            # ... use that number as the MDL
                            str_replace_all(ResultMeasureValue_original, c("\\<"="", "\\*" = "","\\=" = "" )), NA))) %>%
  # preserve the units if they are provided:
  mutate(mdl_units = ifelse(!is.na(mdl_vals), DetectionQuantitationLimitMeasure.MeasureUnitCode, units)) %>%
  mutate(zero = 0,
         half = as.numeric(mdl_vals)/2)

# Using the EPA standard for non-detects, select a random number between zero and HALF the MDL:
funky_vals$epa_value <- with(funky_vals,runif(nrow(funky_vals),zero,half))
funky_vals$epa_value[is.nan(funky_vals$epa_value)]<-NA

funky_vals <- funky_vals %>% select(index, epa_value, mdl_vals, mdl_units) %>%
  filter(!is.na(epa_value))

print(paste(round((nrow(funky_vals))/nrow(aggregated_methods)*100,1),'% of samples had values listed as being below a detection limit'))

# replace "aquasat_value" field with these new values
mdls_added <- aggregated_methods %>%
  left_join(funky_vals,by = "index") %>%
  mutate(aquasat_value = ifelse(index %in% funky_vals$index, epa_value, value),
         aquasat_units = ifelse(index %in% funky_vals$index, mdl_units, units),
         aquasat_comments = ifelse(index %in% funky_vals$index, "Approximated using the EPA's MDL method.", NA))
```

Next step, incorporating and flagging "approximated" values. Using a similar approach to our MDL detection, we can identify value fields that are labelled as being approximated.

```{r}
approximated_vals <- mdls_added %>%
  # First, remove the samples that we've already approximated using the EPA method:
  filter(!index %in% funky_vals$index) %>%
  # Then select fields where the NUMERIC value column is NA....
  filter(is.na(value) & 
           # ... AND the original value column has numeric characters...
            grepl("0|1|2|3|4|5|6|7|8|9", ResultMeasureValue_original) &
           # ...AND any of the comment fields have approximation language...
           (grepl("result approx|RESULT IS APPROX|value approx", lab_comments, ignore.case = T)|
            grepl("result approx|RESULT IS APPROX|value approx", result_comments, ignore.case = T )|
            grepl("result approx|RESULT IS APPROX|value approx", ResultDetectionConditionText, ignore.case = T)))

approximated_vals$approx_value <- as.numeric(str_replace_all(approximated_vals$ResultMeasureValue_original, c("\\*" = "")))
approximated_vals$approx_value[is.nan(approximated_vals$approx_value)] <- NA

approximated_vals <- approximated_vals %>% select(approx_value,index)

print(paste(round((nrow(approximated_vals))/nrow(aggregated_methods)*100,3),'% of samples had values listed as approximated'))

# replace aquasat_value field with these new values
mdl_approx_added <- mdls_added %>%
  left_join(approximated_vals, by="index") %>%
  mutate(aquasat_value=ifelse(index %in% approximated_vals$index, approx_value, aquasat_value)) %>%
  mutate(aquasat_comments=ifelse(index %in% approximated_vals$index, 'Value identified as "approximated" by organization.', aquasat_comments))
```

"Greater than" values. Replace the value with the listed "greater than" value (very rare for TSS in WQP):

```{r}
greater_vals <- mdl_approx_added %>%
  filter((!index %in% funky_vals$index) & (!index %in% approximated_vals$index)) %>%
  # Then select fields where the NUMERIC value column is NA....
  filter(is.na(value) & 
           # ... AND the original value column has numeric characters...
            grepl("0|1|2|3|4|5|6|7|8|9", ResultMeasureValue_original) &
           #... AND a `>` symbol
            grepl(">", ResultMeasureValue_original))

greater_vals$greater_value <- as.numeric(str_replace_all(greater_vals$ResultMeasureValue_original, c("\\>" = "", "\\*" = "", "\\=" = "" )))
greater_vals$greater_value[is.nan(greater_vals$approx_value)] <- NA

greater_vals <- greater_vals %>% select(greater_value,index)

print(paste(round((nrow(greater_vals))/nrow(aggregated_methods)*100,9),'% of samples had values listed as being above a detection limit//greater than'))

# replace aquasat_value field with these new values
mdl_approx_plus_added <- mdl_approx_added %>%
  left_join(greater_vals, by="index") %>%
  mutate(aquasat_value=ifelse(index %in% greater_vals$index, greater_value, aquasat_value)) %>%
  mutate(aquasat_comments=ifelse(index %in% greater_vals$index, 'Value identified as being greater than listed value.', aquasat_comments))
```

### Harmonizing units

```{r}
#Set up a lookup table so that final units are all in ug/L... 
tss_lookup <- tibble(units=c('mg/L','mg/l','ppm','ug/l','ug/L','mg/m3','ppb','mg/cm3','ug/ml','mg/ml','ppt','umol/L','g/l'),
                        conversion = c(1000,1000,1000,1,1,1,1,1000000,1000,1000000,0.000001,60.080000,1000000))

tss_harmonized <- mdl_approx_plus_added %>%
  # drop nonsensical units using an inner join
  inner_join(tss_lookup,by='units') %>%
  # so I don't have to make changes to the tss_lookup, I'm just converting ug/l to mg/l here:
  mutate(harmonized_value=(aquasat_value*conversion)/1000,
         harmonized_unit='mg/L') %>%
# Remove TSS values that are whacked out. I'm using 1000 mg/L based on original light-partitioning workflow (update from aquasatv1)
  # ... which subsequently removed any NA data.
  filter(harmonized_value >= 0 & harmonized_value <= 1000) 

```

### Exploring the final dataset

```{r}
# are my above text-manipulating/value-filling steps doing what I want them to do?
test <- tss_harmonized %>% select(index, value, units, ResultMeasureValue_original, harmonized_value, lab_comments, result_comments, field_comments)

ggplot(data=tss_harmonized) +
  geom_histogram(aes(x=(harmonized_value), fill=method_status), bins=100) +
  scale_x_log10(breaks = trans_breaks("log10", function(x) 10^x),
                labels = trans_format("log10", math_format(10^.x))) +
  facet_wrap(~method_status, scales='fixed') +
  xlab('TSS mg/L') +
  theme_bw() +
  theme(legend.position = "none",
        text = element_text(size = 20))
```

END
